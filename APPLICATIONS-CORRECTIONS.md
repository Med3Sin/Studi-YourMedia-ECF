# Corrections et Am√©liorations des Applications

Ce document recense les corrections et am√©liorations apport√©es aux diff√©rentes applications et composants du projet YourM√©dia. Il sert de r√©f√©rence pour comprendre les modifications effectu√©es et les probl√®mes r√©solus.

## Table des mati√®res

1. [Workflows GitHub Actions](#workflows-github-actions)
   - [Correction de la num√©rotation des workflows](#correction-de-la-num√©rotation-des-workflows)
   - [Mise √† jour des r√©f√©rences aux workflows dans la documentation](#mise-√†-jour-des-r√©f√©rences-aux-workflows-dans-la-documentation)
   - [Correction des param√®tres d'entr√©e du workflow d'infrastructure](#correction-des-param√®tres-dentr√©e-du-workflow-dinfrastructure)
   - [Mise √† jour des instructions d√©taill√©es pour chaque workflow](#mise-√†-jour-des-instructions-d√©taill√©es-pour-chaque-workflow)
   - [Automatisation du stockage des outputs Terraform dans les secrets GitHub](#automatisation-du-stockage-des-outputs-terraform-dans-les-secrets-github)

2. [Backend (Java)](#backend-java)
   - [Configuration de l'utilisateur SSH pour le d√©ploiement](#configuration-de-lutilisateur-ssh-pour-le-d√©ploiement)

3. [Infrastructure](#infrastructure)
   - [Correction de la configuration du cycle de vie du bucket S3](#correction-de-la-configuration-du-cycle-de-vie-du-bucket-s3)
   - [Configuration de Grafana/Prometheus dans des conteneurs Docker sur EC2](#configuration-de-grafanaprometheus-dans-des-conteneurs-docker-sur-ec2)
   - [Correction de l'erreur de r√©f√©rence √† ECS dans le module de monitoring](#correction-de-lerreur-de-r√©f√©rence-√†-ecs-dans-le-module-de-monitoring)
   - [Suppression du fichier docker-compose.yml.tpl redondant](#suppression-du-fichier-docker-composeyml-tpl-redondant)
   - [Correction des variables manquantes dans le module de monitoring](#correction-des-variables-manquantes-dans-le-module-de-monitoring)
   - [Cr√©ation d'un VPC et de sous-r√©seaux d√©di√©s](#cr√©ation-dun-vpc-et-de-sous-r√©seaux-d√©di√©s)

4. [Documentation](#documentation)
   - [Mise √† jour de la documentation du module de monitoring](#mise-√†-jour-de-la-documentation-du-module-de-monitoring)
   - [Ajout de la documentation sur la configuration SSH](#ajout-de-la-documentation-sur-la-configuration-ssh)
   - [Mise √† jour des r√©f√©rences √† ECS dans la documentation](#mise-√†-jour-des-r√©f√©rences-√†-ecs-dans-la-documentation)

## Workflows GitHub Actions

### Correction de la num√©rotation des workflows

#### Probl√®me identifi√©
Les workflows GitHub Actions avaient une num√©rotation incoh√©rente, avec des fichiers nomm√©s `0-infra-deploy-destroy.yml`, `3-backend-deploy.yml` et `3-frontend-deploy.yml`.

#### Solution mise en ≈ìuvre
Renommage des fichiers de workflow pour avoir une num√©rotation coh√©rente et logique :
1. `0-infra-deploy-destroy.yml` ‚Üí `1-infra-deploy-destroy.yml`
2. `3-backend-deploy.yml` ‚Üí `2-backend-deploy.yml`
3. `3-frontend-deploy.yml` ‚Üí `3-frontend-deploy.yml`

#### Avantages de cette solution
- **Coh√©rence** : Num√©rotation logique et s√©quentielle des workflows
- **Clart√©** : Meilleure compr√©hension de l'ordre d'ex√©cution recommand√©
- **Maintenabilit√©** : Facilite l'ajout de nouveaux workflows √† l'avenir

### Mise √† jour des r√©f√©rences aux workflows dans la documentation

#### Probl√®me identifi√©
Les r√©f√©rences aux workflows dans la documentation (README.md et autres fichiers) ne correspondaient pas aux nouveaux noms des fichiers de workflow.

#### Solution mise en ≈ìuvre
Mise √† jour de toutes les r√©f√©rences aux workflows dans la documentation pour refl√©ter la nouvelle num√©rotation :
- Remplacement de `0-infra-deploy-destroy.yml` par `1-infra-deploy-destroy.yml`
- Remplacement de `3-backend-deploy.yml` par `2-backend-deploy.yml`
- Remplacement de `3-frontend-deploy.yml` par `3-frontend-deploy.yml`

#### Avantages de cette solution
- **Coh√©rence** : Documentation align√©e avec le code r√©el
- **Clart√©** : Instructions pr√©cises pour les utilisateurs
- **√âvite la confusion** : Pr√©vient les erreurs lors de l'utilisation des workflows

### Correction des param√®tres d'entr√©e du workflow d'infrastructure

#### Probl√®me identifi√©
Le workflow d'infrastructure (`1-infra-deploy-destroy.yml`) avait des param√®tres d'entr√©e redondants et incoh√©rents. Certains param√®tres √©taient demand√©s √† l'utilisateur alors qu'ils pouvaient √™tre r√©cup√©r√©s automatiquement.

#### Solution mise en ≈ìuvre
1. Suppression des param√®tres d'entr√©e redondants (`repo_owner` et `repo_name`)
2. Utilisation des variables contextuelles GitHub (`github.repository_owner` et `github.repository`)
3. Simplification des variables d'environnement AWS en utilisant l'action `aws-actions/configure-aws-credentials`

#### Avantages de cette solution
- **Simplicit√©** : Moins de param√®tres √† saisir pour l'utilisateur
- **Fiabilit√©** : Utilisation des valeurs correctes garantie par GitHub
- **S√©curit√©** : Meilleure gestion des identifiants AWS

### Mise √† jour des instructions d√©taill√©es pour chaque workflow

#### Probl√®me identifi√©
La documentation ne contenait pas d'instructions d√©taill√©es et √† jour pour l'utilisation des workflows GitHub Actions. Les sections correspondantes √©taient marqu√©es comme "*(Instructions pour utiliser le workflow `X-workflow.yml`)*" sans contenu r√©el.

#### Solution mise en ≈ìuvre
1. Ajout d'instructions d√©taill√©es pour le workflow d'infrastructure (`1-infra-deploy-destroy.yml`)
   - √âtapes pr√©cises pour d√©ployer ou d√©truire l'infrastructure
   - Explication des param√®tres d'entr√©e
   - Ordre logique des √©tapes √† suivre

2. Ajout d'instructions d√©taill√©es pour le workflow de d√©ploiement backend (`2-backend-deploy.yml`)
   - Pr√©requis pour le d√©ploiement
   - √âtapes pr√©cises pour d√©ployer l'application Java
   - Informations sur l'acc√®s √† l'application d√©ploy√©e

3. Ajout d'instructions d√©taill√©es pour le workflow de d√©ploiement frontend (`3-frontend-deploy.yml`)
   - Explication du r√¥le du workflow (v√©rification CI uniquement)
   - Clarification sur le d√©ploiement automatique via AWS Amplify
   - Instructions pour acc√©der √† l'application d√©ploy√©e

#### Avantages de cette solution
- **Clart√©** : Instructions pr√©cises et d√©taill√©es pour chaque workflow
- **Facilit√© d'utilisation** : R√©duction des erreurs lors de l'utilisation des workflows
- **Autonomie** : Permet aux utilisateurs de d√©ployer l'application sans assistance

### Automatisation du stockage des outputs Terraform dans les secrets GitHub

#### Probl√®me identifi√©
Les workflows de d√©ploiement des applications n√©cessitaient la saisie manuelle des informations d'infrastructure (adresse IP de l'EC2, nom du bucket S3, etc.) √† chaque ex√©cution. Ces informations √©taient disponibles dans les outputs Terraform, mais n'√©taient pas automatiquement accessibles aux autres workflows.

#### Solution mise en ≈ìuvre
1. **Modification du workflow d'infrastructure** (`1-infra-deploy-destroy.yml`) :
   - Ajout d'une √©tape pour r√©cup√©rer les outputs Terraform apr√®s l'application de l'infrastructure
   - Stockage de ces outputs dans des variables d'environnement GitHub Actions
   - Cr√©ation de secrets GitHub √† partir de ces variables d'environnement

2. **Modification du workflow de d√©ploiement backend** (`2-backend-deploy.yml`) :
   - Rendus optionnels les param√®tres d'entr√©e (adresse IP de l'EC2, nom du bucket S3)
   - Ajout d'une √©tape pour r√©cup√©rer les informations depuis les secrets GitHub si disponibles
   - Utilisation d'une logique de fallback : utiliser les secrets s'ils existent, sinon utiliser les param√®tres d'entr√©e

3. **Mise √† jour de la documentation** :
   - Ajout d'informations sur les secrets cr√©√©s automatiquement
   - Mise √† jour des instructions de d√©ploiement pour refl√©ter cette automatisation

#### Avantages de cette solution
- **Automatisation** : R√©duction des √©tapes manuelles pour le d√©ploiement des applications
- **Fiabilit√©** : √âlimination des erreurs de saisie lors du d√©ploiement
- **Coh√©rence** : Utilisation des m√™mes valeurs dans tous les workflows
- **Flexibilit√©** : Possibilit√© de fournir manuellement les param√®tres si n√©cessaire

## Backend (Java)

### Configuration de l'utilisateur SSH pour le d√©ploiement

#### Probl√®me identifi√©
Le workflow de d√©ploiement du backend (`2-backend-deploy.yml`) utilisait l'utilisateur `ubuntu` pour se connecter √† l'instance EC2, alors que l'AMI Amazon Linux 2 utilise l'utilisateur `ec2-user`.

#### Solution mise en ≈ìuvre
Modification du workflow pour utiliser l'utilisateur `ec2-user` au lieu de `ubuntu` dans la commande SSH :
```yaml
ssh ec2-user@${{ github.event.inputs.ec2_public_ip }} << EOF
```

#### Avantages de cette solution
- **Compatibilit√©** : Fonctionne correctement avec l'AMI Amazon Linux 2
- **Fiabilit√©** : √âvite les erreurs de connexion SSH
- **Coh√©rence** : Alignement avec la configuration de l'instance EC2

## Infrastructure

### Correction de la configuration du cycle de vie du bucket S3

#### Probl√®me identifi√©
Lors de l'application de l'infrastructure avec Terraform, l'erreur suivante √©tait rencontr√©e :
```
Error: "filter" or "prefix" is required in rule[0] of lifecycle_rule
```

Cette erreur indique que la configuration du cycle de vie du bucket S3 n√©cessite soit un filtre, soit un pr√©fixe pour chaque r√®gle.

#### Solution mise en ≈ìuvre
Nous avons mis √† jour la configuration du cycle de vie du bucket S3 pour inclure un filtre vide, ce qui applique la r√®gle √† tous les objets du bucket :

```hcl
lifecycle_rule {
  id      = "expire-all-objects"
  enabled = true

  filter {} # Filtre vide = s'applique √† tous les objets

  expiration {
    days = 1
  }
}
```

Cette modification satisfait l'exigence du provider AWS Terraform tout en maintenant le comportement d'origine (application de la r√®gle √† tous les objets).

#### Avantages de cette solution
- **Conformit√©** : Satisfait les exigences du provider AWS Terraform
- **Compatibilit√© future** : Assure la compatibilit√© avec les futures versions du provider
- **Maintien du comportement** : Conserve le comportement d'origine (application de la r√®gle √† tous les objets)

### Configuration de Grafana/Prometheus dans des conteneurs Docker sur EC2

#### Probl√®me identifi√©
La configuration initiale utilisait ECS Fargate pour d√©ployer Grafana et Prometheus, ce qui n'√©tait pas optimal pour rester dans les limites du Free Tier AWS. De plus, les services Grafana et Prometheus n'√©taient pas accessibles aux URLs attendues.

#### Solution mise en ≈ìuvre
Nous avons modifi√© l'infrastructure pour d√©ployer Grafana et Prometheus dans des conteneurs Docker sur une instance EC2 d√©di√©e au monitoring :

1. **Cr√©ation d'un script d'initialisation** pour l'instance EC2 qui installe Docker et configure les conteneurs Grafana et Prometheus
2. **Modification du module ecs-monitoring** pour utiliser une instance EC2 au lieu de ECS Fargate
3. **Exposition des ports** 3000 (Grafana) et 9090 (Prometheus) sur l'instance EC2
4. **Mise √† jour des outputs Terraform** pour exposer les URLs de Grafana et Prometheus

#### Avantages de cette solution
- **√âconomie de co√ªts** : Utilisation d'une seule instance EC2 au lieu de services ECS Fargate, ce qui est plus √©conomique et reste dans les limites du Free Tier AWS
- **Simplicit√©** : Configuration plus simple et plus directe avec Docker
- **Flexibilit√©** : Possibilit√© de personnaliser facilement la configuration des conteneurs
- **Performances** : Meilleure performance pour les services de monitoring

### Correction de l'erreur de r√©f√©rence √† ECS dans le module de monitoring

#### Probl√®me identifi√©
Apr√®s la migration de ECS Fargate vers Docker sur EC2 pour le monitoring, une erreur √©tait rencontr√©e lors de l'application de l'infrastructure :

```
Error: Reference to undeclared resource
  on modules/ecs-monitoring/ec2-capacity.tf line 60, in resource "aws_instance" "ecs_instance":
  60:     echo ECS_CLUSTER=${aws_ecs_cluster.monitoring_cluster.name} >> /etc/ecs/ecs.config
A managed resource "aws_ecs_cluster" "monitoring_cluster" has not been
declared in module.ecs-monitoring.
```

Cette erreur indique qu'il y avait encore des r√©f√©rences √† des ressources ECS qui n'existaient plus dans le module de monitoring.

#### Solution mise en ≈ìuvre
Nous avons nettoy√© le module de monitoring en supprimant les fichiers et r√©f√©rences obsol√®tes :

1. **Suppression du fichier `ec2-capacity.tf`** qui contenait des r√©f√©rences √† ECS
2. **Suppression des fichiers de d√©finition de t√¢ches ECS** qui n'√©taient plus n√©cessaires
3. **Mise √† jour du README du module** pour refl√©ter la nouvelle architecture Docker

#### Avantages de cette solution
- **Coh√©rence** : √âlimination des r√©f√©rences obsol√®tes pour √©viter les erreurs
- **Clart√©** : Documentation mise √† jour pour refl√©ter l'architecture actuelle
- **Simplicit√©** : R√©duction du nombre de fichiers et de ressources pour une meilleure maintenabilit√©

### Suppression du fichier docker-compose.yml.tpl redondant

#### Probl√®me identifi√©
Le module de monitoring contenait deux fichiers docker-compose quasiment identiques : `docker-compose.yml` et `docker-compose.yml.tpl`. Cette redondance cr√©ait de la confusion et des erreurs lors du d√©ploiement.

#### Solution mise en ≈ìuvre
1. **Analyse des fichiers** pour confirmer qu'ils √©taient identiques en contenu
2. **V√©rification du code Terraform** pour identifier quel fichier √©tait r√©ellement utilis√©
3. **Suppression du fichier redondant** `docker-compose.yml.tpl`
4. **Mise √† jour des r√©f√©rences** dans le code Terraform pour utiliser uniquement `docker-compose.yml`

#### Avantages de cette solution
- **R√©duction de la complexit√©** : Moins de fichiers √† maintenir
- **√âlimination de la confusion** : Un seul fichier docker-compose √† modifier
- **Coh√©rence** : Assure que les modifications futures seront appliqu√©es au bon fichier

### Correction des variables manquantes dans le module de monitoring

#### Probl√®me identifi√©
Apr√®s la migration de ECS vers Docker sur EC2, le module `ecs-monitoring` n√©cessitait de nouvelles variables qui n'√©taient pas fournies dans l'appel du module, ce qui provoquait l'erreur suivante lors de la validation Terraform :

```
Error: Missing required argument
  on main.tf line 78, in module "ecs-monitoring":
  78: module "ecs-monitoring" {
The argument "key_pair_name" is required, but no definition was found.
```

#### Solution mise en ≈ìuvre
1. **Ajout des variables manquantes** dans l'appel au module `ecs-monitoring` dans le fichier `main.tf` :
   - `key_pair_name` : Nom de la paire de cl√©s SSH pour l'instance EC2 de monitoring
   - `ssh_private_key_path` : Chemin vers la cl√© priv√©e SSH pour se connecter √† l'instance EC2

2. **Ajout d'une nouvelle variable** dans le fichier `variables.tf` principal :
   - `ssh_private_key_path` : Chemin vers la cl√© priv√©e SSH

3. **Mise √† jour du workflow GitHub Actions** pour fournir la valeur de `ssh_private_key_path` lors de l'ex√©cution de Terraform

4. **Mise √† jour des outputs** dans le fichier `outputs.tf` pour refl√©ter la nouvelle architecture Docker sur EC2 :
   - Remplacement de `ecs_cluster_name` par `monitoring_ec2_public_ip`, `grafana_url` et `prometheus_url`

5. **Correction du script d'initialisation** pour √©viter les probl√®mes d'encodage UTF-8 en utilisant un template Terraform local au lieu d'un fichier externe

#### Avantages de cette solution
- **Coh√©rence** : Toutes les variables n√©cessaires sont maintenant fournies
- **Fiabilit√©** : √âvite les erreurs lors de l'ex√©cution de Terraform
- **Simplicit√©** : Utilisation d'un template local pour le script d'initialisation, √©vitant les probl√®mes d'encodage
- **Clart√©** : Outputs plus descriptifs et coh√©rents avec l'architecture actuelle

### Cr√©ation d'un VPC et de sous-r√©seaux d√©di√©s

#### Probl√®me identifi√©
Lors de l'ex√©cution de `terraform plan`, des erreurs apparaissaient concernant l'impossibilit√© de trouver les sous-r√©seaux sp√©cifi√©s :

```
Error: no matching EC2 Subnet found

  with data.aws_subnet.default_az1,
  on main.tf line 11, in data "aws_subnet" "default_az1":
  11: data "aws_subnet" "default_az1" {


Error: no matching EC2 Subnet found

  with data.aws_subnet.default_az2,
  on main.tf line 17, in data "aws_subnet" "default_az2":
  17: data "aws_subnet" "default_az2" {
```

Ces erreurs √©taient dues au fait que la configuration tentait de trouver des sous-r√©seaux sp√©cifiques dans le VPC par d√©faut, mais ces sous-r√©seaux n'existaient pas ou ne correspondaient pas aux crit√®res sp√©cifi√©s.

#### Solution mise en ≈ìuvre
1. **Cr√©ation d'un VPC d√©di√© au projet** :
   - Cr√©ation d'un nouveau VPC avec un bloc CIDR `10.0.0.0/16`
   - Activation du support DNS et des noms d'h√¥tes DNS

2. **Cr√©ation de sous-r√©seaux dans une seule zone de disponibilit√©** :
   - Cr√©ation de deux sous-r√©seaux dans la m√™me zone de disponibilit√© (`eu-west-3a`)
   - Configuration des sous-r√©seaux pour attribuer automatiquement des adresses IP publiques

3. **Configuration de l'acc√®s Internet** :
   - Cr√©ation d'une Internet Gateway
   - Cr√©ation d'une table de routage avec une route par d√©faut vers Internet
   - Association de la table de routage aux deux sous-r√©seaux

4. **Mise √† jour des r√©f√©rences dans les modules** :
   - Modification de toutes les r√©f√©rences au VPC par d√©faut pour utiliser le nouveau VPC
   - Modification de toutes les r√©f√©rences aux sous-r√©seaux par d√©faut pour utiliser les nouveaux sous-r√©seaux

#### Avantages de cette solution
- **Contr√¥le total** : Contr√¥le complet sur la configuration du VPC et des sous-r√©seaux
- **Isolation** : Isolation des ressources du projet dans un VPC d√©di√©
- **Optimisation des co√ªts** : Utilisation d'une seule zone de disponibilit√© pour rester dans les limites du Free Tier AWS
- **Simplicit√©** : Configuration claire et explicite sans d√©pendance aux ressources par d√©faut d'AWS
- **Reproductibilit√©** : Infrastructure enti√®rement d√©finie dans le code, facilitant la reproduction dans diff√©rents environnements

## Documentation

### Mise √† jour de la documentation du module de monitoring

#### Probl√®me identifi√©
La documentation du module de monitoring (`infrastructure/modules/ecs-monitoring/README.md`) faisait r√©f√©rence √† l'ancienne architecture bas√©e sur ECS Fargate, ce qui ne correspondait plus √† la nouvelle impl√©mentation bas√©e sur Docker sur EC2.

#### Solution mise en ≈ìuvre
Mise √† jour compl√®te du README du module de monitoring pour refl√©ter la nouvelle architecture :
1. **Mise √† jour de la description** du module
2. **Mise √† jour de la liste des ressources cr√©√©es**
3. **Mise √† jour des fichiers de configuration**
4. **Mise √† jour des variables d'entr√©e**
5. **Mise √† jour des sorties**
6. **Mise √† jour des instructions d'acc√®s** √† Grafana et Prometheus
7. **Ajout d'une section sur les optimisations Free Tier**

#### Avantages de cette solution
- **Clart√©** : Documentation pr√©cise et √† jour
- **Facilit√© d'utilisation** : Instructions claires pour acc√©der aux services
- **Transparence** : Explication des choix d'optimisation pour le Free Tier

### Ajout de la documentation sur la configuration SSH

#### Probl√®me identifi√©
La documentation ne contenait pas d'instructions claires sur la configuration des cl√©s SSH pour le d√©ploiement du backend sur l'instance EC2.

#### Solution mise en ≈ìuvre
Ajout d'une nouvelle section dans la documentation principale (README.md) sur la configuration SSH :
1. **Instructions pour g√©n√©rer une paire de cl√©s SSH** sur diff√©rents syst√®mes d'exploitation
2. **Instructions pour extraire une cl√© publique** √† partir d'une cl√© priv√©e existante
3. **Instructions pour configurer les cl√©s SSH** dans GitHub et AWS
4. **Explication des secrets GitHub** li√©s √† SSH (`EC2_SSH_PRIVATE_KEY`, `EC2_SSH_PUBLIC_KEY`, `EC2_KEY_PAIR_NAME`)
5. **Mise √† jour de la table des mati√®res** pour inclure la nouvelle section

#### Avantages de cette solution
- **Compl√©tude** : Documentation couvrant tous les aspects du d√©ploiement
- **Clart√©** : Instructions √©tape par √©tape pour la configuration SSH
- **Facilit√© d'utilisation** : R√©duction des erreurs lors du d√©ploiement

### Mise √† jour des r√©f√©rences √† ECS dans la documentation

#### Probl√®me identifi√©
La documentation faisait encore r√©f√©rence √† ECS pour le monitoring, alors que l'architecture avait √©t√© modifi√©e pour utiliser Docker sur EC2.

#### Solution mise en ≈ìuvre
1. **Mise √† jour du titre de la section** de "Monitoring (ECS avec EC2 - Prometheus & Grafana)" √† "Monitoring (Docker sur EC2 - Prometheus & Grafana)"
2. **Mise √† jour de la description de l'architecture** pour mentionner les conteneurs Docker sur EC2 au lieu d'ECS
3. **Mise √† jour des instructions d'acc√®s** √† Grafana et Prometheus
4. **Mise √† jour des liens dans la table des mati√®res**

#### Avantages de cette solution
- **Coh√©rence** : Documentation align√©e avec l'architecture r√©elle
- **Pr√©cision** : √âvite la confusion sur la technologie utilis√©e
- **Clart√©** : Instructions correctes pour acc√©der aux services
# #   F r o n t e n d   ( R e a c t   N a t i v e   W e b ) 
 
 # # #   C o r r e c t i o n   d u   p r o b l Ë m e   d e   d È p e n d a n c e s 
 
 # # # #   P r o b l Ë m e   i d e n t i f i È 
 L o r s   d e   l ' e x È c u t i o n   d u   w o r k f l o w   G i t H u b   A c t i o n s   p o u r   l e   f r o n t e n d ,   l ' e r r e u r   s u i v a n t e   È t a i t   r e n c o n t r È e   : 
 \ \ \ 
 E r r o r :   D e p e n d e n c i e s   l o c k   f i l e   i s   n o t   f o u n d   i n   / h o m e / r u n n e r / w o r k / S t u d i - Y o u r M e d i a - E C F / S t u d i - Y o u r M e d i a - E C F .   S u p p o r t e d   f i l e   p a t t e r n s :   p a c k a g e - l o c k . j s o n , n p m - s h r i n k w r a p . j s o n , y a r n . l o c k 
 \ \ \ 
 
 C e t t e   e r r e u r   i n d i q u e   q u e   l e   f i c h i e r   d e   v e r r o u i l l a g e   d e s   d È p e n d a n c e s   ( p a c k a g e - l o c k . j s o n   o u   y a r n . l o c k )   n ' a   p a s   È t È   t r o u v È   d a n s   l e   r È p e r t o i r e   d u   p r o j e t . 
 
 # # # #   S o l u t i o n   m i s e   e n   Su v r e 
 1 .   * * G È n È r a t i o n   d u   f i c h i e r   p a c k a g e - l o c k . j s o n * *   e n   e x È c u t a n t   \ 
 p m   i n s t a l l \   d a n s   l e   r È p e r t o i r e   a p p - r e a c t 
 2 .   * * C o m m i t   d u   f i c h i e r   p a c k a g e - l o c k . j s o n * *   d a n s   l e   d È p Ù t   G i t 
 3 .   * * M i s e   ‡   j o u r   d u   w o r k f l o w   G i t H u b   A c t i o n s * *   p o u r   u t i l i s e r   l e   f i c h i e r   p a c k a g e - l o c k . j s o n 
 
 # # # #   A v a n t a g e s   d e   c e t t e   s o l u t i o n 
 -   * * F i a b i l i t È * *   :   G a r a n t i t   q u e   l e s   m Í m e s   v e r s i o n s   d e   d È p e n d a n c e s   s o n t   u t i l i s È e s   d a n s   t o u s   l e s   e n v i r o n n e m e n t s 
 -   * * R e p r o d u c t i b i l i t È * *   :   A s s u r e   q u e   l e s   b u i l d s   s o n t   r e p r o d u c t i b l e s 
 -   * * S t a b i l i t È * *   :   … v i t e   l e s   p r o b l Ë m e s   l i È s   a u x   m i s e s   ‡   j o u r   a u t o m a t i q u e s   d e   d È p e n d a n c e s  
 