name: 1 - Deploy/Destroy Infrastructure (Terraform)

# -----------------------------------------------------------------------
# Ce workflow gère le déploiement et la destruction de l'infrastructure AWS via Terraform.
# Il utilise le secret GH_PAT au lieu de GITHUB_TOKEN car les noms de secrets personnalisés
# ne doivent pas commencer par GITHUB_ (préfixe réservé aux variables d'environnement GitHub).
# La syntaxe ${{ secrets.GH_PAT || '' }} permet d'utiliser une chaîne vide si le secret
# n'est pas défini, évitant ainsi le blocage du workflow.
# -----------------------------------------------------------------------

on:
  workflow_dispatch:
    inputs:
      action:
        description: 'Action Terraform à exécuter'
        required: true
        default: 'plan'
        type: choice
        options:
          - plan     # Prévisualiser les changements sans les appliquer
          - apply    # Appliquer les changements à l'infrastructure
          - destroy  # Détruire l'infrastructure existante
      environment:
        description: 'Environnement de déploiement'
        required: true
        default: 'dev'
        type: choice
        options:
          - dev      # Environnement de développement
          - pre-prod # Environnement de pré-production
          - prod     # Environnement de production



# Variables d'environnement globales
env:
  TF_WORKING_DIR: ./infrastructure  # Répertoire contenant les fichiers Terraform
  AWS_REGION: eu-west-3            # Région AWS (Paris)
  AWS_DEFAULT_REGION: eu-west-3    # Région AWS par défaut (Paris)

jobs:
  terraform:
    name: 'Terraform ${{ github.event.inputs.action }}'
    runs-on: ubuntu-latest
    defaults:
      run:
        working-directory: ${{ env.TF_WORKING_DIR }}

    steps:
      # Étape 1: Récupération du code source
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          ref: main  # Utilise explicitement la branche main
          fetch-depth: 0  # Récupère tout l'historique pour les tags

      # Étape 2: Configuration de Terraform
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: ">=1.0"  # Version compatible avec le code
          terraform_wrapper: false    # Désactive le wrapper pour une meilleure gestion des erreurs
          cli_config_credentials_token: ${{ secrets.TF_API_TOKEN }} # Token pour Terraform Cloud

      # Étape 3: Configuration des identifiants AWS
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-region: ${{ env.AWS_REGION }}

      # Étape 3.5: Configuration des identifiants Docker Hub
      - name: Login to Docker Hub
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKERHUB_TOKEN }}

      # Étape 4: Configuration des variables d'environnement et des clés SSH
      - name: Configure Environment Variables and SSH Keys
        id: config
        run: |
          # Définir les variables d'environnement AWS
          echo "AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}" >> $GITHUB_ENV
          echo "AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}" >> $GITHUB_ENV
          echo "AWS_DEFAULT_REGION=${{ env.AWS_REGION }}" >> $GITHUB_ENV

          # Créer un fichier de configuration pour l'API Terraform Cloud
          cat > ~/.terraformrc << EOF
          credentials "app.terraform.io" {
            token = "${{ secrets.TF_API_TOKEN }}"
          }
          EOF

          # Créer un fichier .env pour Terraform Cloud
          cat > .env << EOF
          AWS_ACCESS_KEY_ID=${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY=${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION=${{ env.AWS_REGION }}
          EOF

          # Configuration de la clé SSH si disponible
          if [ ! -z "${{ secrets.EC2_SSH_PRIVATE_KEY }}" ]; then
            mkdir -p ~/.ssh
            echo "${{ secrets.EC2_SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
            chmod 600 ~/.ssh/id_rsa
            echo "SSH_KEY_CONFIGURED=true" >> $GITHUB_ENV
            echo "Clé SSH privée configurée."
          else
            echo "SSH_KEY_CONFIGURED=false" >> $GITHUB_ENV
            echo "Aucune clé SSH privée configurée."
          fi

      # Étape 5: Initialisation de Terraform
      - name: Terraform Init
        id: init
        run: |
          # Exporter les variables d'environnement AWS
          export AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}"
          export AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          export AWS_DEFAULT_REGION="${{ env.AWS_REGION }}"

          # Initialiser Terraform
          terraform init

      # Étape 5: Validation de la configuration Terraform
      - name: Terraform Validate
        id: validate
        run: terraform validate -no-color

      # Étape 6: Formatage du code Terraform (vérification uniquement)
      - name: Terraform Format Check
        id: fmt
        run: terraform fmt -check -recursive
        continue-on-error: true  # Ne bloque pas le workflow si le formatage n'est pas correct

      # Étape 7: Planification des changements Terraform
      - name: Terraform Plan
        id: plan
        if: github.event.inputs.action == 'plan' || github.event.inputs.action == 'apply'
        run: |
          echo "::group::Terraform Plan"
          # Créer le fichier de clé SSH si le secret est disponible
          if [ ! -z "${{ secrets.EC2_SSH_PRIVATE_KEY }}" ]; then
            mkdir -p ~/.ssh
            echo "${{ secrets.EC2_SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
            chmod 600 ~/.ssh/id_rsa
            echo "Clé SSH privée configurée."
          fi

          # Exporter les variables d'environnement AWS
          export AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}"
          export AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          export AWS_DEFAULT_REGION="${{ env.AWS_REGION }}"

          terraform plan \
            -var="aws_access_key=${{ secrets.AWS_ACCESS_KEY_ID }}" \
            -var="aws_secret_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            -var="db_username=${{ secrets.RDS_USERNAME }}" \
            -var="db_password=${{ secrets.RDS_PASSWORD }}" \
            -var="db_name=${{ secrets.DB_NAME || 'yourmedia' }}" \
            -var="ec2_key_pair_name=${{ secrets.EC2_KEY_PAIR_NAME }}" \
            -var="github_token=${{ secrets.GH_PAT || '' }}" \
            -var="repo_owner=${{ github.repository_owner }}" \
            -var="repo_name=${{ github.repository }}" \
            -var="ssh_private_key_path=~/.ssh/id_rsa" \
            -var="ssh_private_key_content='${{ secrets.EC2_SSH_PRIVATE_KEY || '' }}'" \
            -var="ssh_public_key='${{ secrets.EC2_SSH_PUBLIC_KEY || '' }}'" \
            -var="enable_provisioning=${{ secrets.EC2_SSH_PRIVATE_KEY != '' }}" \
            -var="environment=${{ github.event.inputs.environment }}" \
            -var="tf_api_token=${{ secrets.TF_API_TOKEN }}" \
            -var="tf_workspace_id=${{ secrets.TF_WORKSPACE_ID }}" \
            -var="dockerhub_username=${{ secrets.DOCKER_USERNAME }}" \
            -var="dockerhub_token=${{ secrets.DOCKERHUB_TOKEN }}" \
            -var="dockerhub_repo=${{ secrets.DOCKER_REPO }}" \
            -no-color \
            -out=tfplan
          echo "::endgroup::"
        continue-on-error: false  # Arrête le workflow si le plan échoue

      # Étape 7.5: Vérification et création du bucket S3 si nécessaire
      - name: Ensure S3 Bucket Exists
        id: ensure_s3
        if: github.event.inputs.action == 'apply'
        run: |
          echo "::group::Vérification et création du bucket S3"
          # Exporter les variables d'environnement AWS
          export AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}"
          export AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          export AWS_DEFAULT_REGION="${{ env.AWS_REGION }}"

          # Vérifier si le secret TF_S3_BUCKET_NAME est disponible
          if [ ! -z "${{ secrets.TF_S3_BUCKET_NAME }}" ]; then
            echo "Secret TF_S3_BUCKET_NAME trouvé: ${{ secrets.TF_S3_BUCKET_NAME }}"
            S3_BUCKET_NAME="${{ secrets.TF_S3_BUCKET_NAME }}"

            # Vérifier si le bucket existe réellement
            if aws s3api head-bucket --bucket $S3_BUCKET_NAME 2>/dev/null; then
              echo "Le bucket S3 existe: $S3_BUCKET_NAME"
            else
              echo "Le bucket S3 n'existe pas malgré le secret. Création nécessaire..."
              S3_BUCKET_NAME=""
            fi
          else
            echo "Secret TF_S3_BUCKET_NAME non trouvé. Création nécessaire..."
            S3_BUCKET_NAME=""
          fi

          # Si le bucket n'existe pas, le créer via Terraform
          if [ -z "$S3_BUCKET_NAME" ]; then
            echo "Création du bucket S3 via Terraform..."
            # Appliquer uniquement le module S3
            terraform apply -auto-approve -target=module.s3 \
              -var="aws_access_key=${{ secrets.AWS_ACCESS_KEY_ID }}" \
              -var="aws_secret_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
              -var="db_username=${{ secrets.RDS_USERNAME }}" \
              -var="db_password=${{ secrets.RDS_PASSWORD }}" \
              -var="db_name=${{ secrets.DB_NAME || 'yourmedia' }}" \
              -var="ec2_key_pair_name=${{ secrets.EC2_KEY_PAIR_NAME }}" \
              -var="github_token=${{ secrets.GH_PAT || '' }}" \
              -var="repo_owner=${{ github.repository_owner }}" \
              -var="repo_name=${{ github.repository }}" \
              -var="ssh_private_key_path=~/.ssh/id_rsa" \
              -var="ssh_private_key_content='${{ secrets.EC2_SSH_PRIVATE_KEY || '' }}'" \
              -var="ssh_public_key='${{ secrets.EC2_SSH_PUBLIC_KEY || '' }}'" \
              -var="enable_provisioning=${{ secrets.EC2_SSH_PRIVATE_KEY != '' }}" \
              -var="environment=${{ github.event.inputs.environment || 'dev' }}" \
              -var="tf_api_token=${{ secrets.TF_API_TOKEN }}" \
              -var="tf_workspace_id=${{ secrets.TF_WORKSPACE_ID }}" \
              -var="dockerhub_username=${{ secrets.DOCKER_USERNAME || '' }}" \
              -var="dockerhub_token=${{ secrets.DOCKERHUB_TOKEN || '' }}" \
              -var="dockerhub_repo=${{ secrets.DOCKER_REPO || '' }}"

            # Récupérer le nom du bucket S3 après la création
            S3_BUCKET_NAME=$(terraform output -raw s3_bucket_name)

            if [ -z "$S3_BUCKET_NAME" ]; then
              echo "ERREUR: Impossible de récupérer le nom du bucket S3 après création."
              exit 1
            fi

            echo "Bucket S3 créé avec succès: $S3_BUCKET_NAME"
          fi

          # Stocker le nom du bucket pour les étapes suivantes
          echo "S3_BUCKET_NAME=$S3_BUCKET_NAME" >> $GITHUB_ENV
          echo "::endgroup::"

      # Étape 7.6: Vérifier si le secret doit être mis à jour
      - name: Check if Secret Needs Update
        id: check_secret
        if: github.event.inputs.action == 'apply' && env.S3_BUCKET_NAME != ''
        run: |
          # Stocker la valeur du secret dans une variable
          SECRET_VALUE="${{ secrets.TF_S3_BUCKET_NAME }}"

          # Comparer avec la valeur actuelle du bucket S3
          if [ "$SECRET_VALUE" != "${{ env.S3_BUCKET_NAME }}" ]; then
            echo "UPDATE_SECRET=true" >> $GITHUB_ENV
            echo "Le secret TF_S3_BUCKET_NAME doit être mis à jour."
          else
            echo "UPDATE_SECRET=false" >> $GITHUB_ENV
            echo "Le secret TF_S3_BUCKET_NAME est déjà à jour."
          fi

      # Étape 7.7: Mise à jour du secret GitHub avec le nom du bucket S3
      - name: Update S3 Bucket Name Secret
        id: update_secret
        if: github.event.inputs.action == 'apply' && env.S3_BUCKET_NAME != '' && env.UPDATE_SECRET == 'true'
        env:
          GH_TOKEN: ${{ secrets.GH_PAT }}
          SECRET_NAME: TF_S3_BUCKET_NAME
          SECRET_VALUE: ${{ env.S3_BUCKET_NAME }}
          REPO: ${{ github.repository }}
        run: |
          # Récupérer la clé publique du dépôt
          echo "Récupération de la clé publique pour le dépôt '$REPO'..."
          PUBLIC_KEY_RESPONSE=$(curl -s -X GET \
            -H "Authorization: token $GH_TOKEN" \
            -H "Accept: application/vnd.github.v3+json" \
            "https://api.github.com/repos/$REPO/actions/secrets/public-key")

          # Extraire la clé publique et son ID
          KEY_ID=$(echo "$PUBLIC_KEY_RESPONSE" | jq -r '.key_id')
          PUBLIC_KEY=$(echo "$PUBLIC_KEY_RESPONSE" | jq -r '.key')

          if [ -z "$KEY_ID" ] || [ "$KEY_ID" == "null" ]; then
            echo "Erreur: Impossible de récupérer la clé publique. Réponse: $PUBLIC_KEY_RESPONSE"
            exit 1
          fi

          echo "Clé publique récupérée avec succès."

          # Installer les dépendances nécessaires pour le chiffrement
          echo "Installation des dépendances pour le chiffrement..."
          sudo apt-get update -qq
          sudo apt-get install -y -qq python3-pip libsodium-dev
          pip3 install -q pynacl

          # Chiffrer la valeur du secret
          echo "Chiffrement de la valeur du secret..."
          # Écrire la valeur du secret dans un fichier temporaire pour éviter les problèmes d'échappement
          echo "$SECRET_VALUE" > /tmp/secret_value.txt

          ENCRYPTED_VALUE=$(python3 -c "
          import base64
          import json
          from nacl import encoding, public

          def encrypt(public_key, secret_value):
              public_key = public.PublicKey(public_key.encode('utf-8'), encoding.Base64Encoder())
              sealed_box = public.SealedBox(public_key)
              encrypted = sealed_box.encrypt(secret_value.encode('utf-8'))
              return base64.b64encode(encrypted).decode('utf-8')

          with open('/tmp/secret_value.txt', 'r') as f:
              secret_value = f.read()

          print(encrypt('$PUBLIC_KEY', secret_value))
          ")

          # Supprimer le fichier temporaire
          rm -f /tmp/secret_value.txt

          # Mettre à jour le secret
          echo "Mise à jour du secret '$SECRET_NAME'..."
          curl -s -X PUT \
            -H "Authorization: token $GH_TOKEN" \
            -H "Accept: application/vnd.github.v3+json" \
            -H "Content-Type: application/json" \
            -d "{\"encrypted_value\":\"$ENCRYPTED_VALUE\",\"key_id\":\"$KEY_ID\"}" \
            "https://api.github.com/repos/$REPO/actions/secrets/$SECRET_NAME"

          echo "Secret '$SECRET_NAME' mis à jour avec succès."

# Cette étape a été déplacée après la création du bucket S3 et avant le déploiement de l'infrastructure

      # Étape 8: Application des changements Terraform en deux phases
      - name: Terraform Apply - Phase 1 (S3 Bucket)
        id: apply_phase1
        if: github.event.inputs.action == 'apply'
        run: |
          echo "::group::Terraform Apply - Phase 1 (S3 Bucket)"
          # Exporter les variables d'environnement AWS
          export AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}"
          export AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          export AWS_DEFAULT_REGION="${{ env.AWS_REGION }}"

          # Réinitialiser Terraform pour s'assurer que l'état est à jour
          echo "Réinitialisation de Terraform..."
          terraform init

          # Phase 1: Créer uniquement le bucket S3
          echo "Phase 1: Création du bucket S3..."
          terraform apply -auto-approve -target=module.s3 \
            -var="aws_access_key=${{ secrets.AWS_ACCESS_KEY_ID }}" \
            -var="aws_secret_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            -var="db_username=${{ secrets.RDS_USERNAME }}" \
            -var="db_password=${{ secrets.RDS_PASSWORD }}" \
            -var="db_name=${{ secrets.DB_NAME || 'yourmedia' }}" \
            -var="ec2_key_pair_name=${{ secrets.EC2_KEY_PAIR_NAME }}" \
            -var="github_token=${{ secrets.GH_PAT || '' }}" \
            -var="repo_owner=${{ github.repository_owner }}" \
            -var="repo_name=${{ github.repository }}" \
            -var="ssh_private_key_path=~/.ssh/id_rsa" \
            -var="ssh_private_key_content='${{ secrets.EC2_SSH_PRIVATE_KEY || '' }}'" \
            -var="ssh_public_key='${{ secrets.EC2_SSH_PUBLIC_KEY || '' }}'" \
            -var="enable_provisioning=${{ secrets.EC2_SSH_PRIVATE_KEY != '' }}" \
            -var="environment=${{ github.event.inputs.environment }}" \
            -var="tf_api_token=${{ secrets.TF_API_TOKEN }}" \
            -var="tf_workspace_id=${{ secrets.TF_WORKSPACE_ID }}" \
            -var="dockerhub_username=${{ secrets.DOCKER_USERNAME }}" \
            -var="dockerhub_token=${{ secrets.DOCKERHUB_TOKEN }}" \
            -var="dockerhub_repo=${{ secrets.DOCKER_REPO }}"

          # Récupérer le nom du bucket S3
          S3_BUCKET_NAME=$(terraform output -raw s3_bucket_name)
          echo "S3_BUCKET_NAME=$S3_BUCKET_NAME" >> $GITHUB_ENV
          echo "Nom du bucket S3 défini comme variable d'environnement: $S3_BUCKET_NAME"

          echo "Bucket S3 créé avec succès: $S3_BUCKET_NAME"
          echo "::endgroup::"

      # Étape 8.1: Attente pour s'assurer que le bucket S3 est disponible
      - name: Wait for S3 Bucket Availability
        id: wait_s3
        if: github.event.inputs.action == 'apply'
        run: |
          echo "::group::Attente de la disponibilité du bucket S3"
          echo "Attente de 30 secondes pour s'assurer que le bucket S3 est disponible..."
          sleep 30

          # Vérifier que le bucket est accessible
          aws s3 ls s3://${S3_BUCKET_NAME} || (echo "ERREUR: Le bucket S3 n'est pas accessible" && exit 1)
          echo "Bucket S3 vérifié et accessible."
          echo "::endgroup::"

      # Étape 8.1.5: Téléchargement des scripts dans S3
      - name: Upload Scripts to S3
        id: upload_scripts
        if: github.event.inputs.action == 'apply'
        run: |
          echo "::group::Téléchargement des scripts dans S3"
          # Créer le fichier de clé SSH si le secret est disponible
          if [ ! -z "${{ secrets.EC2_SSH_PRIVATE_KEY }}" ]; then
            mkdir -p ~/.ssh
            echo "${{ secrets.EC2_SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
            chmod 600 ~/.ssh/id_rsa
            echo "Clé SSH privée configurée."
          fi

          echo "Téléchargement des scripts dans le bucket S3: ${S3_BUCKET_NAME}"

          # Définir le chemin absolu vers le répertoire de travail
          WORKSPACE_DIR="${GITHUB_WORKSPACE}"
          echo "Répertoire de travail: ${WORKSPACE_DIR}"

          # Vérifier que les répertoires existent
          echo "Vérification des répertoires de scripts..."
          ls -la ${WORKSPACE_DIR}
          ls -la ${WORKSPACE_DIR}/scripts || echo "Répertoire scripts non trouvé"

          # Télécharger les scripts de monitoring
          if [ -d "${WORKSPACE_DIR}/scripts/ec2-monitoring" ]; then
            echo "Téléchargement des scripts de monitoring..."
            aws s3 cp --recursive ${WORKSPACE_DIR}/scripts/ec2-monitoring/ s3://${S3_BUCKET_NAME}/scripts/ec2-monitoring/

            # Vérifier et télécharger les scripts d'amélioration de surveillance
            echo "Vérification des scripts d'amélioration de surveillance..."
            if [ -f "${WORKSPACE_DIR}/scripts/ec2-monitoring/container-health-check.sh" ]; then
              echo "Téléchargement des scripts d'amélioration de surveillance..."
              # S'assurer que les scripts sont exécutables
              chmod +x ${WORKSPACE_DIR}/scripts/ec2-monitoring/container-health-check.sh
              chmod +x ${WORKSPACE_DIR}/scripts/ec2-monitoring/container-tests.sh
              chmod +x ${WORKSPACE_DIR}/scripts/ec2-monitoring/setup-monitoring-improvements.sh

              # Télécharger les scripts individuellement pour s'assurer qu'ils sont correctement téléchargés
              aws s3 cp ${WORKSPACE_DIR}/scripts/ec2-monitoring/container-health-check.sh s3://${S3_BUCKET_NAME}/scripts/ec2-monitoring/
              aws s3 cp ${WORKSPACE_DIR}/scripts/ec2-monitoring/container-health-check.service s3://${S3_BUCKET_NAME}/scripts/ec2-monitoring/
              aws s3 cp ${WORKSPACE_DIR}/scripts/ec2-monitoring/container-health-check.timer s3://${S3_BUCKET_NAME}/scripts/ec2-monitoring/
              aws s3 cp ${WORKSPACE_DIR}/scripts/ec2-monitoring/container-tests.sh s3://${S3_BUCKET_NAME}/scripts/ec2-monitoring/
              aws s3 cp ${WORKSPACE_DIR}/scripts/ec2-monitoring/container-tests.service s3://${S3_BUCKET_NAME}/scripts/ec2-monitoring/
              aws s3 cp ${WORKSPACE_DIR}/scripts/ec2-monitoring/container-tests.timer s3://${S3_BUCKET_NAME}/scripts/ec2-monitoring/
              aws s3 cp ${WORKSPACE_DIR}/scripts/ec2-monitoring/loki-config.yml s3://${S3_BUCKET_NAME}/scripts/ec2-monitoring/
              aws s3 cp ${WORKSPACE_DIR}/scripts/ec2-monitoring/promtail-config.yml s3://${S3_BUCKET_NAME}/scripts/ec2-monitoring/
              aws s3 cp ${WORKSPACE_DIR}/scripts/ec2-monitoring/setup-monitoring-improvements.sh s3://${S3_BUCKET_NAME}/scripts/ec2-monitoring/

              # Créer le répertoire prometheus-rules dans S3 s'il n'existe pas
              aws s3api put-object --bucket ${S3_BUCKET_NAME} --key scripts/ec2-monitoring/prometheus-rules/

              # Télécharger les règles Prometheus
              if [ -d "${WORKSPACE_DIR}/scripts/ec2-monitoring/prometheus-rules" ]; then
                aws s3 cp --recursive ${WORKSPACE_DIR}/scripts/ec2-monitoring/prometheus-rules/ s3://${S3_BUCKET_NAME}/scripts/ec2-monitoring/prometheus-rules/
              fi

              echo "Scripts d'amélioration de surveillance téléchargés avec succès."
            else
              echo "AVERTISSEMENT: Les scripts d'amélioration de surveillance n'ont pas été trouvés."
            fi
          else
            echo "AVERTISSEMENT: Le répertoire ${WORKSPACE_DIR}/scripts/ec2-monitoring n'existe pas."
          fi

          # Télécharger les scripts Java/Tomcat
          if [ -d "${WORKSPACE_DIR}/scripts/ec2-java-tomcat" ]; then
            echo "Téléchargement des scripts Java/Tomcat..."
            aws s3 cp --recursive ${WORKSPACE_DIR}/scripts/ec2-java-tomcat/ s3://${S3_BUCKET_NAME}/scripts/ec2-java-tomcat/
          else
            echo "AVERTISSEMENT: Le répertoire ${WORKSPACE_DIR}/scripts/ec2-java-tomcat n'existe pas."
          fi

          # Télécharger les scripts Docker
          if [ -d "${WORKSPACE_DIR}/scripts/docker" ]; then
            echo "Téléchargement des scripts Docker..."
            aws s3 cp --recursive ${WORKSPACE_DIR}/scripts/docker/ s3://${S3_BUCKET_NAME}/scripts/docker/
          else
            echo "AVERTISSEMENT: Le répertoire ${WORKSPACE_DIR}/scripts/docker n'existe pas."
          fi

          echo "Scripts téléchargés avec succès dans le bucket S3: ${S3_BUCKET_NAME}"
          echo "::endgroup::"

      # Étape 8.1.6: Attente supplémentaire pour s'assurer que les scripts sont disponibles
      - name: Wait for Scripts Availability
        id: wait_scripts
        if: github.event.inputs.action == 'apply'
        run: |
          echo "::group::Attente de la disponibilité des scripts"
          echo "Attente de 15 secondes pour s'assurer que les scripts sont disponibles..."
          sleep 15

          # Vérifier que les scripts sont accessibles
          echo "Vérification de la disponibilité des scripts..."
          aws s3 ls s3://${S3_BUCKET_NAME}/scripts/ --recursive
          echo "Scripts vérifiés et accessibles."
          echo "::endgroup::"

      # Étape 8.2: Application des changements Terraform - Phase 2 (Reste de l'infrastructure)
      - name: Terraform Apply - Phase 2 (Infrastructure)
        id: apply_phase2
        if: github.event.inputs.action == 'apply'
        run: |
          echo "::group::Terraform Apply - Phase 2 (Infrastructure)"
          # Exporter les variables d'environnement AWS
          export AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}"
          export AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          export AWS_DEFAULT_REGION="${{ env.AWS_REGION }}"

          # Réinitialiser Terraform pour s'assurer que l'état est à jour
          echo "Réinitialisation de Terraform..."
          terraform init

          # Phase 2: Déployer le reste de l'infrastructure
          echo "Phase 2: Déploiement du reste de l'infrastructure..."
          terraform apply -auto-approve \
            -var="aws_access_key=${{ secrets.AWS_ACCESS_KEY_ID }}" \
            -var="aws_secret_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            -var="db_username=${{ secrets.RDS_USERNAME }}" \
            -var="db_password=${{ secrets.RDS_PASSWORD }}" \
            -var="db_name=${{ secrets.DB_NAME || 'yourmedia' }}" \
            -var="ec2_key_pair_name=${{ secrets.EC2_KEY_PAIR_NAME }}" \
            -var="github_token=${{ secrets.GH_PAT || '' }}" \
            -var="repo_owner=${{ github.repository_owner }}" \
            -var="repo_name=${{ github.repository }}" \
            -var="ssh_private_key_path=~/.ssh/id_rsa" \
            -var="ssh_private_key_content='${{ secrets.EC2_SSH_PRIVATE_KEY || '' }}'" \
            -var="ssh_public_key='${{ secrets.EC2_SSH_PUBLIC_KEY || '' }}'" \
            -var="enable_provisioning=${{ secrets.EC2_SSH_PRIVATE_KEY != '' }}" \
            -var="environment=${{ github.event.inputs.environment }}" \
            -var="tf_api_token=${{ secrets.TF_API_TOKEN }}" \
            -var="tf_workspace_id=${{ secrets.TF_WORKSPACE_ID }}" \
            -var="dockerhub_username=${{ secrets.DOCKER_USERNAME }}" \
            -var="dockerhub_token=${{ secrets.DOCKERHUB_TOKEN }}" \
            -var="dockerhub_repo=${{ secrets.DOCKER_REPO }}"
          echo "::endgroup::"

          # Affiche les outputs après l'application
          echo "\n\n--- Outputs de l'infrastructure ---"
          terraform output

          # Attendre que les instances soient prêtes
          echo "::group::Attente que les instances soient prêtes"
          echo "Attente que les instances soient prêtes (60 secondes)..."
          sleep 60
          echo "::endgroup::"

          # Stockage des outputs dans les secrets GitHub
          echo "::group::Stockage des outputs dans les secrets GitHub"
          # Récupération des outputs importants
          # Utilisation de variables locales pour éviter les erreurs si les outputs n'existent pas
          EC2_PUBLIC_IP=$(terraform output -raw ec2_public_ip 2>/dev/null || echo "")
          S3_BUCKET_NAME=$(terraform output -raw s3_bucket_name 2>/dev/null || echo "")
          MONITORING_EC2_PUBLIC_IP=$(terraform output -raw monitoring_ec2_public_ip 2>/dev/null || echo "")
          RDS_ENDPOINT=$(terraform output -raw rds_endpoint 2>/dev/null || echo "")
          GRAFANA_URL=$(terraform output -raw grafana_url 2>/dev/null || echo "")

          # Stockage dans les variables d'environnement du workflow
          if [ ! -z "$EC2_PUBLIC_IP" ]; then
            echo "EC2_PUBLIC_IP=$EC2_PUBLIC_IP" >> $GITHUB_ENV
            echo "ec2_public_ip=$EC2_PUBLIC_IP" >> $GITHUB_OUTPUT
          fi

          if [ ! -z "$S3_BUCKET_NAME" ]; then
            echo "S3_BUCKET_NAME=$S3_BUCKET_NAME" >> $GITHUB_ENV
            echo "s3_bucket_name=$S3_BUCKET_NAME" >> $GITHUB_OUTPUT
          fi

          if [ ! -z "$MONITORING_EC2_PUBLIC_IP" ]; then
            echo "MONITORING_EC2_PUBLIC_IP=$MONITORING_EC2_PUBLIC_IP" >> $GITHUB_ENV
            echo "monitoring_ec2_public_ip=$MONITORING_EC2_PUBLIC_IP" >> $GITHUB_OUTPUT
          fi

          if [ ! -z "$RDS_ENDPOINT" ]; then
            echo "RDS_ENDPOINT=$RDS_ENDPOINT" >> $GITHUB_ENV
            echo "rds_endpoint=$RDS_ENDPOINT" >> $GITHUB_OUTPUT
          fi

          if [ ! -z "$GRAFANA_URL" ]; then
            echo "GRAFANA_URL=$GRAFANA_URL" >> $GITHUB_ENV
            echo "grafana_url=$GRAFANA_URL" >> $GITHUB_OUTPUT
          fi

          # Note: La référence à Amplify a été supprimée car nous utilisons maintenant des conteneurs Docker

          echo "::endgroup::"
        continue-on-error: false  # Arrête le workflow si l'application échoue

      # Étape 8.5: Arrêter et supprimer les conteneurs Docker sur les instances EC2
      - name: Cleanup Docker Containers
        id: cleanup_containers
        if: github.event.inputs.action == 'destroy'
        run: |
          echo "::group::Nettoyage des conteneurs Docker"
          # Récupérer les adresses IP des instances EC2 depuis les outputs Terraform
          EC2_PUBLIC_IP=$(terraform output -raw ec2_public_ip 2>/dev/null || echo "")
          MONITORING_EC2_PUBLIC_IP=$(terraform output -raw monitoring_ec2_public_ip 2>/dev/null || echo "")

          # Créer le fichier de clé SSH si le secret est disponible
          if [ ! -z "${{ secrets.EC2_SSH_PRIVATE_KEY }}" ]; then
            mkdir -p ~/.ssh
            echo "${{ secrets.EC2_SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
            chmod 600 ~/.ssh/id_rsa
            echo "Clé SSH privée configurée."
          fi

          # Exécuter le script de nettoyage des conteneurs
          if [ ! -z "$EC2_PUBLIC_IP" ] && [ ! -z "$MONITORING_EC2_PUBLIC_IP" ]; then
            echo "Nettoyage des conteneurs Docker sur les instances EC2..."
            chmod +x ${GITHUB_WORKSPACE}/scripts/docker/cleanup-containers.sh
            ${GITHUB_WORKSPACE}/scripts/docker/cleanup-containers.sh $MONITORING_EC2_PUBLIC_IP $EC2_PUBLIC_IP ~/.ssh/id_rsa
            echo "Nettoyage des conteneurs Docker terminé."
          else
            echo "Adresses IP des instances EC2 non disponibles. Impossible de nettoyer les conteneurs Docker."
          fi
          echo "::endgroup::"
        continue-on-error: true  # Continuer même si le nettoyage échoue

      # Étape 8.6: Vider le bucket S3 avant la destruction
      - name: Empty S3 Bucket
        id: empty_s3
        if: github.event.inputs.action == 'destroy'
        run: |
          # Exporter les variables d'environnement AWS
          export AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}"
          export AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          export AWS_DEFAULT_REGION="${{ env.AWS_REGION }}"
          echo "::group::Vidage du bucket S3"
          # Récupérer le nom du bucket S3 depuis les outputs Terraform
          S3_BUCKET_NAME=$(terraform output -raw s3_bucket_name 2>/dev/null || echo "")

          if [ ! -z "$S3_BUCKET_NAME" ]; then
            echo "Vidage du bucket S3: $S3_BUCKET_NAME"
            # Vérifier si le bucket existe
            if aws s3api head-bucket --bucket $S3_BUCKET_NAME 2>/dev/null; then
              # Vider le bucket S3
              aws s3 rm s3://$S3_BUCKET_NAME --recursive
              echo "Bucket S3 vidé avec succès."
            else
              echo "Le bucket S3 $S3_BUCKET_NAME n'existe pas ou n'est pas accessible."
            fi
          else
            echo "Aucun bucket S3 à vider ou nom de bucket non disponible."
          fi
          echo "::endgroup::"
        continue-on-error: true  # Continuer même si le vidage échoue

      # Étape 9: Destruction de l'infrastructure Terraform
      - name: Terraform Destroy
        id: destroy
        if: github.event.inputs.action == 'destroy'
        run: |
          echo "::group::Terraform Destroy"
          # Créer le fichier de clé SSH si le secret est disponible
          if [ ! -z "${{ secrets.EC2_SSH_PRIVATE_KEY }}" ]; then
            mkdir -p ~/.ssh
            echo "${{ secrets.EC2_SSH_PRIVATE_KEY }}" > ~/.ssh/id_rsa
            chmod 600 ~/.ssh/id_rsa
            echo "Clé SSH privée configurée."
          fi

          # Exporter les variables d'environnement AWS
          export AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}"
          export AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          export AWS_DEFAULT_REGION="${{ env.AWS_REGION }}"

          terraform destroy -auto-approve \
            -var="aws_access_key=${{ secrets.AWS_ACCESS_KEY_ID }}" \
            -var="aws_secret_key=${{ secrets.AWS_SECRET_ACCESS_KEY }}" \
            -var="db_username=${{ secrets.RDS_USERNAME }}" \
            -var="db_password=${{ secrets.RDS_PASSWORD }}" \
            -var="db_name=${{ secrets.DB_NAME || 'yourmedia' }}" \
            -var="ec2_key_pair_name=${{ secrets.EC2_KEY_PAIR_NAME }}" \
            -var="github_token=${{ secrets.GH_PAT || '' }}" \
            -var="repo_owner=${{ github.repository_owner }}" \
            -var="repo_name=${{ github.repository }}" \
            -var="ssh_private_key_path=~/.ssh/id_rsa" \
            -var="ssh_private_key_content='${{ secrets.EC2_SSH_PRIVATE_KEY || '' }}'" \
            -var="ssh_public_key='${{ secrets.EC2_SSH_PUBLIC_KEY || '' }}'" \
            -var="enable_provisioning=${{ secrets.EC2_SSH_PRIVATE_KEY != '' }}" \
            -var="environment=${{ github.event.inputs.environment }}" \
            -var="tf_api_token=${{ secrets.TF_API_TOKEN }}" \
            -var="tf_workspace_id=${{ secrets.TF_WORKSPACE_ID }}" \
            -var="dockerhub_username=${{ secrets.DOCKER_USERNAME }}" \
            -var="dockerhub_token=${{ secrets.DOCKERHUB_TOKEN }}" \
            -var="dockerhub_repo=${{ secrets.DOCKER_REPO }}"
          echo "::endgroup::"
        continue-on-error: true  # Continuer même si la destruction échoue pour pouvoir nettoyer les ressources persistantes

      # Étape 9.1: Nettoyage des profils IAM persistants
      - name: Cleanup IAM Profiles
        if: github.event.inputs.action == 'destroy'
        run: |
          # Exporter les variables d'environnement AWS
          export AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}"
          export AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          export AWS_DEFAULT_REGION="${{ env.AWS_REGION }}"
          echo "::group::Nettoyage des profils IAM persistants"

          # Définir les noms des ressources IAM à nettoyer
          PROJECT_NAME="yourmedia"
          ENVIRONMENT="${{ github.event.inputs.environment }}"
          EC2_PROFILE="${PROJECT_NAME}-${ENVIRONMENT}-ec2-profile"
          MONITORING_PROFILE="${PROJECT_NAME}-${ENVIRONMENT}-monitoring-profile"
          EC2_ROLE="${PROJECT_NAME}-${ENVIRONMENT}-ec2-role-v2"
          MONITORING_ROLE="${PROJECT_NAME}-${ENVIRONMENT}-monitoring-role-v2"

          echo "Nettoyage des profils IAM pour le projet ${PROJECT_NAME} et l'environnement ${ENVIRONMENT}..."

          # Fonction pour supprimer un profil IAM
          delete_instance_profile() {
            local profile_name=$1
            echo "Suppression du profil IAM $profile_name..."

            # Vérifier si le profil existe
            if aws iam get-instance-profile --instance-profile-name $profile_name 2>/dev/null; then
              # Récupérer les rôles attachés au profil
              ROLES=$(aws iam get-instance-profile --instance-profile-name $profile_name --query "InstanceProfile.Roles[*].RoleName" --output text)

              # Détacher les rôles du profil
              for role in $ROLES; do
                echo "Détachement du rôle $role du profil $profile_name..."
                aws iam remove-role-from-instance-profile --instance-profile-name $profile_name --role-name $role
              done

              # Supprimer le profil
              aws iam delete-instance-profile --instance-profile-name $profile_name
              echo "Profil IAM $profile_name supprimé avec succès."
            else
              echo "Le profil IAM $profile_name n'existe pas."
            fi
          }

          # Supprimer les profils IAM
          delete_instance_profile $EC2_PROFILE
          delete_instance_profile $MONITORING_PROFILE

          echo "Nettoyage des profils IAM terminé."
          echo "::endgroup::"
        continue-on-error: true  # Continuer même si le nettoyage échoue

      # Étape 9.2: Nettoyage des instances EC2 persistantes
      - name: Cleanup EC2 Instances
        if: github.event.inputs.action == 'destroy'
        run: |
          # Exporter les variables d'environnement AWS
          export AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}"
          export AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          export AWS_DEFAULT_REGION="${{ env.AWS_REGION }}"
          echo "::group::Nettoyage des instances EC2 persistantes"

          # Définir les noms des instances EC2 à nettoyer
          PROJECT_NAME="yourmedia"
          ENVIRONMENT="${{ github.event.inputs.environment }}"
          MONITORING_INSTANCE_NAME="${PROJECT_NAME}-${ENVIRONMENT}-monitoring-instance"
          APP_INSTANCE_NAME="${PROJECT_NAME}-${ENVIRONMENT}-app-instance"

          echo "Nettoyage des instances EC2 pour le projet ${PROJECT_NAME} et l'environnement ${ENVIRONMENT}..."

          # Fonction pour supprimer une instance EC2
          terminate_ec2_instance() {
            local instance_name=$1
            echo "Recherche de l'instance EC2 $instance_name..."

            # Récupérer l'ID de l'instance EC2 par son nom
            INSTANCE_IDS=$(aws ec2 describe-instances --filters "Name=tag:Name,Values=$instance_name" "Name=instance-state-name,Values=running,stopped,pending,stopping" --query "Reservations[*].Instances[*].InstanceId" --output text 2>/dev/null)

            if [ ! -z "$INSTANCE_IDS" ]; then
              echo "Terminaison des instances EC2 $instance_name (IDs: $INSTANCE_IDS)..."
              aws ec2 terminate-instances --instance-ids $INSTANCE_IDS

              # Attendre que les instances soient terminées
              echo "Attente de la terminaison des instances..."
              aws ec2 wait instance-terminated --instance-ids $INSTANCE_IDS
              echo "Instances EC2 $instance_name terminées avec succès."
            else
              echo "Aucune instance EC2 $instance_name n'a été trouvée."
            fi
          }

          # Supprimer les instances EC2
          terminate_ec2_instance $MONITORING_INSTANCE_NAME
          terminate_ec2_instance $APP_INSTANCE_NAME

          echo "Nettoyage des instances EC2 terminé."
          echo "::endgroup::"
        continue-on-error: true  # Continuer même si le nettoyage échoue

      # Étape 9.3: Nettoyage des interfaces réseau persistantes
      - name: Cleanup Network Interfaces
        if: github.event.inputs.action == 'destroy'
        run: |
          # Exporter les variables d'environnement AWS
          export AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}"
          export AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          export AWS_DEFAULT_REGION="${{ env.AWS_REGION }}"
          echo "::group::Nettoyage des interfaces réseau persistantes"

          # Récupérer toutes les interfaces réseau disponibles
          echo "Recherche des interfaces réseau disponibles..."
          ENI_IDS=$(aws ec2 describe-network-interfaces --filters "Name=status,Values=available" --query "NetworkInterfaces[*].NetworkInterfaceId" --output text 2>/dev/null)

          if [ ! -z "$ENI_IDS" ]; then
            echo "Interfaces réseau disponibles trouvées: $ENI_IDS"

            # Supprimer chaque interface réseau
            for ENI_ID in $ENI_IDS; do
              echo "Suppression de l'interface réseau $ENI_ID..."
              aws ec2 delete-network-interface --network-interface-id $ENI_ID
              echo "Interface réseau $ENI_ID supprimée avec succès."
            done
          else
            echo "Aucune interface réseau disponible n'a été trouvée."
          fi

          echo "Nettoyage des interfaces réseau terminé."
          echo "::endgroup::"
        continue-on-error: true  # Continuer même si le nettoyage échoue

      # Étape 9.4: Nettoyage des groupes de sécurité persistants
      - name: Cleanup Security Groups
        if: github.event.inputs.action == 'destroy'
        run: |
          # Exporter les variables d'environnement AWS
          export AWS_ACCESS_KEY_ID="${{ secrets.AWS_ACCESS_KEY_ID }}"
          export AWS_SECRET_ACCESS_KEY="${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          export AWS_DEFAULT_REGION="${{ env.AWS_REGION }}"
          echo "::group::Nettoyage des groupes de sécurité persistants"

          # Définir les noms des groupes de sécurité à nettoyer
          PROJECT_NAME="yourmedia"
          ENVIRONMENT="${{ github.event.inputs.environment }}"
          MONITORING_SG="${PROJECT_NAME}-${ENVIRONMENT}-monitoring-sg"
          APP_SG="${PROJECT_NAME}-${ENVIRONMENT}-app-sg"
          RDS_SG="${PROJECT_NAME}-${ENVIRONMENT}-rds-sg"

          echo "Nettoyage des groupes de sécurité pour le projet ${PROJECT_NAME} et l'environnement ${ENVIRONMENT}..."

          # Fonction pour supprimer un groupe de sécurité
          delete_security_group() {
            local sg_name=$1
            echo "Recherche du groupe de sécurité $sg_name..."

            # Récupérer l'ID du groupe de sécurité par son nom
            SG_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=$sg_name" --query "SecurityGroups[0].GroupId" --output text 2>/dev/null)

            if [ "$SG_ID" != "None" ] && [ ! -z "$SG_ID" ]; then
              echo "Suppression du groupe de sécurité $sg_name (ID: $SG_ID)..."

              # Vérifier les dépendances du groupe de sécurité
              echo "Vérification des dépendances du groupe de sécurité..."

              # Vérifier les instances EC2 qui utilisent ce groupe de sécurité
              INSTANCES=$(aws ec2 describe-instances --filters "Name=instance.group-id,Values=$SG_ID" --query "Reservations[*].Instances[*].InstanceId" --output text 2>/dev/null)
              if [ ! -z "$INSTANCES" ]; then
                echo "Instances EC2 utilisant ce groupe de sécurité: $INSTANCES"
                echo "Terminaison des instances EC2..."
                aws ec2 terminate-instances --instance-ids $INSTANCES
                echo "Attente de la terminaison des instances..."
                aws ec2 wait instance-terminated --instance-ids $INSTANCES
              fi

              # Vérifier les interfaces réseau qui utilisent ce groupe de sécurité
              ENIS=$(aws ec2 describe-network-interfaces --filters "Name=group-id,Values=$SG_ID" --query "NetworkInterfaces[*].NetworkInterfaceId" --output text 2>/dev/null)
              if [ ! -z "$ENIS" ]; then
                echo "Interfaces réseau utilisant ce groupe de sécurité: $ENIS"
                for ENI in $ENIS; do
                  echo "Détachement et suppression de l'interface réseau $ENI..."
                  # Vérifier si l'interface est attachée
                  ATTACHMENT=$(aws ec2 describe-network-interfaces --network-interface-ids $ENI --query "NetworkInterfaces[0].Attachment.AttachmentId" --output text 2>/dev/null)
                  if [ "$ATTACHMENT" != "None" ] && [ ! -z "$ATTACHMENT" ]; then
                    echo "Détachement de l'interface réseau $ENI (attachment: $ATTACHMENT)..."
                    aws ec2 detach-network-interface --attachment-id $ATTACHMENT --force
                    # Attendre que l'interface soit détachée
                    sleep 10
                  fi
                  # Supprimer l'interface réseau
                  echo "Suppression de l'interface réseau $ENI..."
                  aws ec2 delete-network-interface --network-interface-id $ENI || true
                done
              fi

              # Supprimer toutes les règles de trafic entrant
              echo "Suppression des règles de trafic entrant..."
              INGRESS_RULES=$(aws ec2 describe-security-groups --group-ids $SG_ID --query 'SecurityGroups[0].IpPermissions' --output json 2>/dev/null)
              if [ "$INGRESS_RULES" != "[]" ] && [ "$INGRESS_RULES" != "null" ]; then
                aws ec2 revoke-security-group-ingress --group-id $SG_ID --ip-permissions "$INGRESS_RULES" 2>/dev/null || true
              fi

              # Supprimer toutes les règles de trafic sortant
              echo "Suppression des règles de trafic sortant..."
              EGRESS_RULES=$(aws ec2 describe-security-groups --group-ids $SG_ID --query 'SecurityGroups[0].IpPermissionsEgress' --output json 2>/dev/null)
              if [ "$EGRESS_RULES" != "[]" ] && [ "$EGRESS_RULES" != "null" ]; then
                aws ec2 revoke-security-group-egress --group-id $SG_ID --ip-permissions "$EGRESS_RULES" 2>/dev/null || true
              fi

              # Attendre un peu pour s'assurer que toutes les dépendances sont supprimées
              echo "Attente de 15 secondes pour s'assurer que toutes les dépendances sont supprimées..."
              sleep 15

              # Supprimer le groupe de sécurité
              echo "Suppression du groupe de sécurité..."
              aws ec2 delete-security-group --group-id $SG_ID || true
              echo "Groupe de sécurité $sg_name supprimé avec succès."
            else
              echo "Le groupe de sécurité $sg_name n'existe pas ou n'a pas pu être trouvé."
            fi
          }

          # Supprimer les groupes de sécurité
          delete_security_group $MONITORING_SG
          delete_security_group $APP_SG
          delete_security_group $RDS_SG

          echo "Nettoyage des groupes de sécurité terminé."
          echo "::endgroup::"
        continue-on-error: true  # Continuer même si le nettoyage échoue

      # Étape 10: Création des secrets GitHub à partir des outputs Terraform
      - name: Update GitHub Secrets
        if: github.event.inputs.action == 'apply'
        run: |
          # Fonction pour mettre à jour un secret GitHub
          update_secret() {
            local secret_name=$1
            local secret_value=$2

            if [ -z "$secret_value" ]; then
              echo "Valeur vide pour $secret_name, secret non mis à jour."
              return
            fi

            echo "Mise à jour du secret $secret_name..."

            # Récupérer la clé publique du dépôt
            response=$(curl -s -X GET \
              -H "Authorization: token ${{ secrets.GH_PAT }}" \
              -H "Accept: application/vnd.github.v3+json" \
              "https://api.github.com/repos/${{ github.repository }}/actions/secrets/public-key")

            # Extraire la clé publique et son ID
            key_id=$(echo "$response" | jq -r '.key_id')
            public_key=$(echo "$response" | jq -r '.key')

            if [ -z "$key_id" ] || [ "$key_id" == "null" ]; then
              echo "Impossible de récupérer la clé publique. Réponse: $response"
              return
            fi

            # Chiffrer la valeur du secret
            encrypted_value=$(python3 -c "
          import base64
          import json
          from nacl import encoding, public

          def encrypt(public_key, secret_value):
              public_key = public.PublicKey(public_key.encode('utf-8'), encoding.Base64Encoder())
              sealed_box = public.SealedBox(public_key)
              encrypted = sealed_box.encrypt(secret_value.encode('utf-8'))
              return base64.b64encode(encrypted).decode('utf-8')

          print(encrypt('$public_key', '$secret_value'))
            ")

            # Mettre à jour le secret
            curl -s -X PUT \
              -H "Authorization: token ${{ secrets.GH_PAT }}" \
              -H "Accept: application/vnd.github.v3+json" \
              -H "Content-Type: application/json" \
              -d "{\"encrypted_value\":\"$encrypted_value\",\"key_id\":\"$key_id\"}" \
              "https://api.github.com/repos/${{ github.repository }}/actions/secrets/$secret_name"

            echo "Secret $secret_name mis à jour avec succès."
          }

          # Installer les dépendances nécessaires
          sudo apt-get update
          sudo apt-get install -y python3-pip libsodium-dev
          pip3 install pynacl

          # Mettre à jour les secrets
          update_secret "TF_EC2_PUBLIC_IP" "$EC2_PUBLIC_IP"
          update_secret "TF_S3_BUCKET_NAME" "$S3_BUCKET_NAME"
          update_secret "TF_MONITORING_EC2_PUBLIC_IP" "$MONITORING_EC2_PUBLIC_IP"
          update_secret "TF_RDS_ENDPOINT" "$RDS_ENDPOINT"
          update_secret "TF_GRAFANA_URL" "$GRAFANA_URL"
        continue-on-error: true

      # Note: La création du secret Amplify App URL a été supprimée car nous utilisons maintenant des conteneurs Docker
      # pour le déploiement du frontend React Native.

      # Étape 11: Synchronisation des secrets GitHub vers Terraform Cloud
      - name: Sync GitHub Secrets to Terraform Cloud
        id: sync_secrets
        if: github.event.inputs.action == 'apply'
        env:
          GITHUB_TOKEN: ${{ secrets.GH_PAT }}
          TF_API_TOKEN: ${{ secrets.TF_API_TOKEN }}
          GITHUB_REPOSITORY: ${{ github.repository }}
          TF_WORKSPACE_ID: ${{ secrets.TF_WORKSPACE_ID }}
          # Exporter les secrets AWS
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ env.AWS_REGION }}
          # Exporter les secrets RDS/DB
          RDS_USERNAME: ${{ secrets.RDS_USERNAME }}
          RDS_PASSWORD: ${{ secrets.RDS_PASSWORD }}
          DB_NAME: ${{ secrets.DB_NAME }}
          DB_USERNAME: ${{ secrets.DB_USERNAME }}
          DB_PASSWORD: ${{ secrets.DB_PASSWORD }}
          # Exporter les secrets SSH et EC2
          EC2_SSH_PRIVATE_KEY: ${{ secrets.EC2_SSH_PRIVATE_KEY }}
          EC2_SSH_PUBLIC_KEY: ${{ secrets.EC2_SSH_PUBLIC_KEY }}
          EC2_KEY_PAIR_NAME: ${{ secrets.EC2_KEY_PAIR_NAME }}
          # Exporter les secrets Docker
          DOCKER_USERNAME: ${{ secrets.DOCKER_USERNAME }}
          DOCKERHUB_USERNAME: ${{ secrets.DOCKERHUB_USERNAME }}
          DOCKERHUB_TOKEN: ${{ secrets.DOCKERHUB_TOKEN }}
          DOCKER_REPO: ${{ secrets.DOCKER_REPO }}
          DOCKERHUB_REPO: ${{ secrets.DOCKERHUB_REPO }}
          # Exporter les secrets Grafana
          GF_SECURITY_ADMIN_PASSWORD: ${{ secrets.GF_SECURITY_ADMIN_PASSWORD }}
          # Exporter les secrets GitHub
          GH_PAT: ${{ secrets.GH_PAT }}
          # Ajouter les variables d'infrastructure
          TF_EC2_PUBLIC_IP: ${{ env.EC2_PUBLIC_IP }}
          TF_S3_BUCKET_NAME: ${{ env.S3_BUCKET_NAME }}
          TF_MONITORING_EC2_PUBLIC_IP: ${{ env.MONITORING_EC2_PUBLIC_IP }}
          TF_RDS_ENDPOINT: ${{ env.RDS_ENDPOINT }}
          TF_GRAFANA_URL: ${{ env.GRAFANA_URL }}
        run: |
          echo "::group::Synchronisation des secrets GitHub vers Terraform Cloud"
          # Rendre le script exécutable
          chmod +x ${GITHUB_WORKSPACE}/scripts/utils/sync-github-secrets-to-terraform.sh

          # Exécuter le script de synchronisation
          ${GITHUB_WORKSPACE}/scripts/utils/sync-github-secrets-to-terraform.sh

          echo "Secrets synchronisés avec succès vers Terraform Cloud"
          echo "::endgroup::"
        continue-on-error: true

      # Étape 12: Résumé de l'exécution
      - name: Summary
        run: |
          echo "## Résumé de l'exécution Terraform" >> $GITHUB_STEP_SUMMARY
          echo "* **Action exécutée:** ${{ github.event.inputs.action }}" >> $GITHUB_STEP_SUMMARY

          if [[ "${{ github.event.inputs.action }}" == "apply" ]]; then
            echo "* **Outputs stockés dans les secrets GitHub:**" >> $GITHUB_STEP_SUMMARY
            if [ ! -z "$EC2_PUBLIC_IP" ]; then echo "  - TF_EC2_PUBLIC_IP" >> $GITHUB_STEP_SUMMARY; fi
            if [ ! -z "$S3_BUCKET_NAME" ]; then echo "  - TF_S3_BUCKET_NAME" >> $GITHUB_STEP_SUMMARY; fi
            if [ ! -z "$MONITORING_EC2_PUBLIC_IP" ]; then echo "  - TF_MONITORING_EC2_PUBLIC_IP" >> $GITHUB_STEP_SUMMARY; fi
            if [ ! -z "$RDS_ENDPOINT" ]; then echo "  - TF_RDS_ENDPOINT" >> $GITHUB_STEP_SUMMARY; fi
            if [ ! -z "$GRAFANA_URL" ]; then echo "  - TF_GRAFANA_URL" >> $GITHUB_STEP_SUMMARY; fi
            # Note: La référence à Amplify a été supprimée car nous utilisons maintenant des conteneurs Docker
          fi
          echo "* **Branche:** main" >> $GITHUB_STEP_SUMMARY
          echo "* **Région AWS:** ${{ env.AWS_REGION }}" >> $GITHUB_STEP_SUMMARY
          echo "* **Statut:** Succès ✅" >> $GITHUB_STEP_SUMMARY
